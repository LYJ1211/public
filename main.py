# main.py
# - ìœ„ë¹„í‹°(ë„¤ì´ë°/ìŠ¬ë¡œê±´) í¬ë¡¤ë§ + GitHub JSON ì—…ì„œíŠ¸
# - data/wevity_naming.json ê°€ 'ì´ë¯¸ ì¡´ì¬'í•´ì•¼ ë™ì‘
# - ì‹ ê·œ í•­ëª©ì— added_at(KST, ISO) ê¸°ë¡(ì•Œë¦¼ ë©”ì‹œì§€ì—ëŠ” í‘œì‹œ X)
# - í…”ë ˆê·¸ë¨ ì•Œë¦¼(HTML / ë§í¬ë¯¸ë¦¬ë³´ê¸° on)
# - ì‹ ê·œ ì•Œë¦¼ì€ ì˜¤ë˜ëœ ê²ƒë¶€í„°(ìµœì‹ ì´ ë§¨ ë§ˆì§€ë§‰ì— ì˜¤ë„ë¡)
# - ì €ì¥ ìˆœì„œ: "ì›¹ì— ë³´ì´ëŠ” í˜„ì¬ ìˆœì„œ"ë¥¼ JSONì˜ ë§¨ ì•(head)ì— ìœ ì§€

import os
import re, time, json, base64, html
from typing import List, Dict, Tuple, Optional
from urllib.parse import urljoin, urlparse, parse_qs
from datetime import datetime, timezone, timedelta

import requests
from requests.adapters import HTTPAdapter
from urllib3.util import Retry
import cloudscraper
from bs4 import BeautifulSoup

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê¸°ë³¸ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CONFIG = {
    "OWNER":  "LYJ1211",
    "REPO":   "public",
    "PATH":   "data/wevity_naming.json",
    "BRANCH": "main",
    "TOKEN":  (os.getenv("GH_PAT") or os.getenv("GITHUB_TOKEN") or "").strip(),
}

TG = {
    "BOT_TOKEN": os.getenv("TG_BOT_TOKEN", "").strip(),
    "CHAT_IDS": [c.strip() for c in os.getenv("TG_CHAT_IDS", "").split(",") if c.strip()],
}
TG_PER_CHAT_INTERVAL_SEC = float(os.getenv("TG_PER_CHAT_INTERVAL_SEC", "3.2"))

# í¬ë¡¤ë§ ë²”ìœ„/ëŒ€ê¸°
SCRAPE_PAGE_FROM = 1
SCRAPE_PAGE_TO   = 3
SCRAPE_DELAY_SEC = 1.6

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í¬ë¡¤ë§
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE = "https://www.wevity.com/"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/124.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    "Cache-Control": "no-cache",
    "Pragma": "no-cache",
    "Upgrade-Insecure-Requests": "1",
    "Referer": "https://www.wevity.com/",
}

_session = requests.Session()
_session.headers.update(HEADERS)
_session.mount(
    "https://",
    HTTPAdapter(max_retries=Retry(
        total=3, backoff_factor=0.7, status_forcelist=[429, 500, 502, 503, 504]
    ))
)

_scraper = cloudscraper.create_scraper(
    browser={'browser': 'chrome', 'platform': 'windows', 'mobile': False}
)

def _get_html(url: str) -> Optional[str]:
    try:
        r = _session.get(url, timeout=20)
        if r.status_code == 200 and "ms-list" in r.text:
            return r.text
        print(f"[WARN] HTTP {r.status_code} @ {url} (requests)")
    except Exception as e:
        print(f"[WARN] requests ì˜ˆì™¸: {e}")
    try:
        print("[INFO] cloudscraper í´ë°± ì‹œë„")
        r2 = _scraper.get(url, headers=HEADERS, timeout=25)
        if r2.status_code == 200:
            return r2.text
        print(f"[WARN] HTTP {r2.status_code} @ {url} (cloudscraper)")
    except Exception as e:
        print(f"[WARN] cloudscraper ì˜ˆì™¸: {e}")
    return None

def _parse_int(s: Optional[str]) -> Optional[int]:
    if not s:
        return None
    s = s.replace(",", "").strip()
    m = re.search(r"-?\d+", s)
    return int(m.group()) if m else None

def _extract_ix_from_url(u: str) -> Optional[str]:
    try:
        q = parse_qs(urlparse(u).query)
        v = q.get("ix", [None])[0]
        return str(v) if v else None
    except Exception:
        return None

def _parse_day_and_status(li) -> Tuple[Optional[int], Optional[str]]:
    day_div = li.select_one("div.day")
    if not day_div:
        return None, None
    dday = None
    m = re.search(r"D-?\s*(\d+)", day_div.get_text(" ", strip=True))
    if m:
        dday = int(m.group(1))
    st = None
    sp = day_div.select_one("span.dday")
    if sp:
        st = sp.get_text(strip=True)
    else:
        text = day_div.get_text(" ", strip=True)
        if "ë§ˆê°" in text:
            st = "ë§ˆê°"
    return dday, st

def _parse_item(li) -> Optional[Dict]:
    a = li.select_one("div.tit a")
    if not a:
        return None
    title = a.get_text(strip=True)
    href  = a.get("href", "")
    url   = urljoin(BASE, href)
    ix    = _extract_ix_from_url(url)

    organizer = li.select_one("div.organ")
    organizer = organizer.get_text(strip=True) if organizer else None

    dday, status = _parse_day_and_status(li)

    views_div = li.select_one("div.read")
    views = _parse_int(views_div.get_text(strip=True)) if views_div else None

    sub = li.select_one("div.sub-tit")
    category = None
    if sub:
        txt = sub.get_text(" ", strip=True)
        category = txt.split(":", 1)[1].strip() if ":" in txt else txt

    return {
        "title": title, "url": url, "ix": ix or url,
        "organizer": organizer, "dday": dday, "status": status,
        "views": views, "category": category,
    }

def scrape_wevity_naming(frm: int, to: int, delay_sec: float) -> List[Dict]:
    out: List[Dict] = []
    for gp in range(frm, to + 1):
        url = f"{BASE}?c=find&s=1&gub=1&cidx=25&gp={gp}"
        html_text = _get_html(url)
        if not html_text:
            time.sleep(delay_sec)
            continue
        soup = BeautifulSoup(html_text, "html.parser")
        lis = soup.select("div.ms-list ul.list > li")
        items = [li for li in lis if "top" not in (li.get("class") or [])]
        if not items:
            print(f"[WARN] ëª©ë¡ ë¯¸ê²€ì¶œ. ì„ íƒì í™•ì¸ í•„ìš”: {url}")
        for li in items:
            it = _parse_item(li)
            if it:
                out.append(it)
        time.sleep(delay_sec)
    return out

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GitHub Contents API
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _api_url() -> str:
    return f"https://api.github.com/repos/{CONFIG['OWNER']}/{CONFIG['REPO']}/contents/{CONFIG['PATH']}"

def _headers() -> Dict:
    return {
        "Authorization": f"Bearer {CONFIG['TOKEN']}",
        "Accept": "application/vnd.github+json",
        "X-GitHub-Api-Version": "2022-11-28",
    }

def _normalize_ix(item: Dict) -> str:
    ix = item.get("ix") or item.get("url")
    if not ix:
        raise ValueError("ê° í•­ëª©ì—ëŠ” ìµœì†Œ 'ix' ë˜ëŠ” 'url'ì´ í•„ìš”")
    return str(ix).strip()

def _get_current_strict() -> Tuple[str, List[Dict]]:
    params = {"ref": CONFIG["BRANCH"]} if CONFIG.get("BRANCH") else None
    r = requests.get(_api_url(), headers=_headers(), params=params, timeout=20)
    if r.status_code == 404:
        raise FileNotFoundError(
            f"GitHubì— '{CONFIG['PATH']}' íŒŒì¼ì´ ì—†ìŒ (ë¦¬í¬: {CONFIG['OWNER']}/{CONFIG['REPO']}, ë¸Œëœì¹˜: {CONFIG['BRANCH']})"
        )
    r.raise_for_status()
    data = r.json()
    content_b64 = data.get("content", "")
    text = base64.b64decode(content_b64).decode("utf-8") if content_b64 else "[]"
    try:
        arr = json.loads(text) if text.strip() else []
    except json.JSONDecodeError:
        arr = []
    return data["sha"], arr

def _put_json(new_list: List[Dict], prev_sha: str, message: str) -> Dict:
    body = {
        "message": message,
        "content": base64.b64encode(
            json.dumps(new_list, ensure_ascii=False, indent=2).encode("utf-8")
        ).decode("utf-8"),
        "sha": prev_sha,
    }
    if CONFIG.get("BRANCH"):
        body["branch"] = CONFIG["BRANCH"]
    r = requests.put(_api_url(), headers=_headers(), json=body, timeout=25)
    r.raise_for_status()
    return r.json()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë³‘í•©: ì‚¬ì´íŠ¸ì—ì„œ ë³¸ í˜„ì¬ ë…¸ì¶œ ìˆœì„œë¥¼ JSON ë§¨ ì•(head)ë¡œ ìœ ì§€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def merge_and_detect_new(
    current: List[Dict],
    scraped: List[Dict],
    *,
    added_at: Optional[str] = None,
    update_existing: bool = False,
):
    # í˜„ì¬ í•­ëª©ì„ ë§µìœ¼ë¡œ
    cur_map: Dict[str, Dict] = { _normalize_ix(x): x for x in current }

    new_items: List[Dict] = []

    # ì‹ ê·œ/ê°±ì‹  ê°ì§€
    for raw in scraped:
        ix = _normalize_ix(raw)
        item = dict(raw)
        item["ix"] = ix
        if ix not in cur_map:
            if "added_at" not in item and added_at:
                item["added_at"] = added_at
            new_items.append(item)
            cur_map[ix] = item
        elif update_existing:
            kept = cur_map[ix].get("added_at")
            cur_map[ix] = {**cur_map[ix], **item}
            if kept is not None:
                cur_map[ix]["added_at"] = kept

    # 1) ë°©ê¸ˆ ë³¸ í˜ì´ì§€ë“¤(ìŠ¤í¬ë©ëœ ìˆœì„œ = ì‚¬ì´íŠ¸ ë…¸ì¶œ ìˆœì„œ)ì„ headë¡œ
    seen = set()
    head: List[Dict] = []
    for raw in scraped:
        ix = _normalize_ix(raw)
        if ix in seen:
            continue
        seen.add(ix)
        head.append(cur_map[ix])  # cur_mapì—ì„œ ìµœì‹  ìƒíƒœë¡œ ì‚½ì…

    # 2) ê·¸ ì™¸ ê³¼ê±° í•­ëª©(tail)ì€ ê¸°ì¡´ currentì˜ ìˆœì„œë¥¼ ìœ ì§€í•˜ë©° ë’¤ì—
    tail = [x for x in current if _normalize_ix(x) not in seen]

    merged = head + tail
    return merged, new_items

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Telegram
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def tg_send_message(token: str, chat_id: str, text: str, *, parse_mode="HTML", disable_preview=False):
    url = f"https://api.telegram.org/bot{token}/sendMessage"
    payload = {
        "chat_id": chat_id,
        "text": text,
        "parse_mode": parse_mode,
        "disable_web_page_preview": disable_preview,
    }
    # ì•ˆí‹°-í”ŒëŸ¬ë“œ ëŒ€ì‘: retry_after ì¡´ì¤‘
    for attempt in range(6):
        r = requests.post(url, json=payload, timeout=20)
        if r.status_code == 429:
            try:
                j = r.json()
                wait = j.get("parameters", {}).get("retry_after", 3)
            except Exception:
                wait = 3
            print(f"[TG] 429 Too Many Requests â†’ {wait}s ëŒ€ê¸° (attempt={attempt+1})")
            time.sleep(wait + 0.5)
            continue
        r.raise_for_status()
        return r.json()
    raise RuntimeError("Telegram: ë°˜ë³µëœ 429ë¡œ ì „ì†¡ ì‹¤íŒ¨")

def notify_telegram(new_items: List[Dict]):
    if not TG["BOT_TOKEN"] or not TG["CHAT_IDS"]:
        print("[ì•Œë¦¼] TG ì„¤ì • ë¹„ì–´ ìˆìŒ â†’ í…”ë ˆê·¸ë¨ ì „ì†¡ ìƒëµ")
        return

    # ì˜¤ë˜ëœ ê²ƒ â†’ ìµœì‹  ìˆœìœ¼ë¡œ ë°œì†¡
    def ix_as_int(x):
        s = str(x.get("ix", ""))
        return int(s) if s.isdigit() else 10**12
    ordered = sorted(new_items, key=ix_as_int)

    for it in ordered:
        title = html.escape(it.get("title", ""))
        url   = it.get("url", "")
        safe_url = html.escape(url, quote=True)

        d     = it.get("dday")
        dtext = f"D-{d}" if isinstance(d, int) else "-"

        organizer = html.escape(it.get("organizer", "") or "-")
        status    = html.escape(it.get("status", "") or "-")
        views     = it.get("views")
        views_txt = f"{views:,}" if isinstance(views, int) else "-"

        text = (
            f"ğŸ“£ <b>{title}</b>\n\n"
            f"ì£¼ìµœ: {organizer}\n"
            f"ìƒíƒœ: {dtext} â€¢ {status} â€¢ ì¡°íšŒ {views_txt}\n\n"
            f"ğŸ”— <a href=\"{safe_url}\">ê³µê³  ë°”ë¡œê°€ê¸°</a>"
        )

        for cid in TG["CHAT_IDS"]:
            tg_send_message(TG["BOT_TOKEN"], cid, text, disable_preview=False)
            time.sleep(TG_PER_CHAT_INTERVAL_SEC)

def notify_print(new_items: List[Dict]):
    def ix_as_int(x):
        s = str(x.get("ix", ""))
        return int(s) if s.isdigit() else 10**12
    ordered = sorted(new_items, key=ix_as_int)
    for it in ordered:
        title = it.get("title", "")
        organizer = it.get("organizer", "") or "-"
        status = it.get("status", "") or "-"
        url = it.get("url", "")
        d = it.get("dday")
        dtext = f"D-{d}" if isinstance(d, int) else "-"
        views_val = it.get("views")
        views_txt = f"{views_val:,}" if isinstance(views_val, int) else "-"
        print("\n".join([
            f"[ì‹ ê·œ] {title}",
            f"  â€¢ ì£¼ìµœ: {organizer}",
            f"  â€¢ ìƒíƒœ: {dtext} â€¢ {status} â€¢ ì¡°íšŒ {views_txt}",
            f"  â€¢ ë§í¬: {url}",
        ]))
        print("-" * 60)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MAIN
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    try:
        sha, current = _get_current_strict()
    except FileNotFoundError as e:
        print(f"[ì¤‘ë‹¨] ëŒ€ìƒ JSON íŒŒì¼ ì—†ìŒ â†’ {e}")
        print("GitHub UIì—ì„œ ë¨¼ì € data/wevity_naming.json ì— '[]' ì €ì¥")
        return
    except requests.HTTPError as e:
        print(f"[ì¤‘ë‹¨] GitHub ìš”ì²­ ì‹¤íŒ¨: {e}")
        return

    scraped = scrape_wevity_naming(SCRAPE_PAGE_FROM, SCRAPE_PAGE_TO, SCRAPE_DELAY_SEC)
    if not scraped:
        print("[ì¤‘ë‹¨] í¬ë¡¤ëŸ¬ ê²°ê³¼ ë¹„ì–´ ìˆìŒ")
        return

    for it in scraped:
        it["ix"] = it.get("ix") or it.get("url")

    now_kst = datetime.now(timezone.utc).astimezone(timezone(timedelta(hours=9)))
    added_stamp = now_kst.isoformat()

    merged, new_items = merge_and_detect_new(
        current, scraped, added_at=added_stamp, update_existing=False
    )

    if new_items:
        notify_print(new_items)
        msg = f"chore: upsert {len(new_items)} new items, total {len(merged)} @ {datetime.now(timezone.utc).isoformat()}"
        try:
            resp = _put_json(merged, sha, msg)
            print(f"GitHub ì €ì¥ ì™„ë£Œ: {resp.get('content',{}).get('path')} sha={resp.get('content',{}).get('sha')}")
            notify_telegram(new_items)
        except requests.HTTPError as e:
            print(f"[ì¤‘ë‹¨] GitHub ì—…ì„œíŠ¸ ì‹¤íŒ¨: {e}")
    else:
        print("ì‹ ê·œ í•­ëª© ì—†ìŒ (ì»¤ë°‹ ì—†ìŒ)")

if __name__ == "__main__":
    main()
