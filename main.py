# wevity_to_github.py -> main.py ë¡œ ì‚¬ìš©
# - ìœ„ë¹„í‹°(ë„¤ì´ë°/ìŠ¬ë¡œê±´) í¬ë¡¤ë§ + GitHub JSON ì—…ì„œíŠ¸
# - data/wevity_naming.json ê°€ 'ì´ë¯¸ ì¡´ì¬'í•´ì•¼ ë™ì‘
# - ì‹ ê·œ í•­ëª©ì— added_at(KST, ISO) ê¸°ë¡(ì•Œë¦¼ ë©”ì‹œì§€ì—ëŠ” í‘œì‹œ X)
# - í…”ë ˆê·¸ë¨ ì•Œë¦¼(HTML / ë§í¬ë¯¸ë¦¬ë³´ê¸° on)
# - ì‹ ê·œ ì•Œë¦¼ì€ ì˜¤ë˜ëœ ê²ƒë¶€í„°(ìµœì‹ ì´ ë§¨ ë§ˆì§€ë§‰ì— ì˜¤ë„ë¡)

import os
import re, time, json, base64, html
from typing import List, Dict, Tuple, Optional
from urllib.parse import urljoin, urlparse, parse_qs
from datetime import datetime, timezone, timedelta

import requests
from requests.adapters import HTTPAdapter
from urllib3.util import Retry
import cloudscraper
from bs4 import BeautifulSoup

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„¤ì •: ìƒˆ ê³„ì •/ë ˆí¬ë¡œ ë°”ê¿” ë„£ê¸°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CONFIG = {
    "OWNER":  "LYJ1211",              # â† ìƒˆ ê¹ƒí—ˆë¸Œ ê³„ì •/ì¡°ì§
    "REPO":   "public",                # â† ë ˆí¬ ì´ë¦„
    "PATH":   "data/wevity_naming.json",
    "BRANCH": "main",
    # í† í°: GH_PAT(ìˆìœ¼ë©´ ìš°ì„ ) -> GITHUB_TOKEN(ì•¡ì…˜ ê¸°ë³¸) ìˆœìœ¼ë¡œ ì‚¬ìš©
    "TOKEN":  (os.getenv("GH_PAT") or os.getenv("GITHUB_TOKEN") or "").strip(),
}

TG = {
    "BOT_TOKEN": os.getenv("TG_BOT_TOKEN", "").strip(),
    "CHAT_IDS": [c.strip() for c in os.getenv("TG_CHAT_IDS", "").split(",") if c.strip()],
}

# ë²”ìœ„/ëŒ€ê¸°(ì°¨ë‹¨ íšŒí”¼ìš©)
SCRAPE_PAGE_FROM = 1
SCRAPE_PAGE_TO   = 3
SCRAPE_DELAY_SEC = 1.6

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í¬ë¡¤ë§
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE = "https://www.wevity.com/"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/124.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    "Cache-Control": "no-cache",
    "Pragma": "no-cache",
    "Upgrade-Insecure-Requests": "1",
    "Referer": "https://www.wevity.com/",
}

# requests ì„¸ì…˜(retry)
_session = requests.Session()
_session.headers.update(HEADERS)
_session.mount(
    "https://",
    HTTPAdapter(max_retries=Retry(
        total=3, backoff_factor=0.7,
        status_forcelist=[429, 500, 502, 503, 504]
    ))
)

# cloudscraper (403 ìš°íšŒ)
_scraper = cloudscraper.create_scraper(
    browser={'browser': 'chrome', 'platform': 'windows', 'mobile': False}
)

def _get_html(url: str) -> Optional[str]:
    try:
        r = _session.get(url, timeout=20)
        if r.status_code == 200 and "ms-list" in r.text:
            return r.text
        print(f"[WARN] HTTP {r.status_code} @ {url} (requests)")
    except Exception as e:
        print(f"[WARN] requests ì˜ˆì™¸: {e}")
    try:
        print("[INFO] cloudscraper í´ë°± ì‹œë„")
        r2 = _scraper.get(url, headers=HEADERS, timeout=25)
        if r2.status_code == 200:
            return r2.text
        print(f"[WARN] HTTP {r2.status_code} @ {url} (cloudscraper)")
    except Exception as e:
        print(f"[WARN] cloudscraper ì˜ˆì™¸: {e}")
    return None

def _parse_int(s: Optional[str]) -> Optional[int]:
    if not s: return None
    s = s.replace(",", "").strip()
    m = re.search(r"-?\d+", s)
    return int(m.group()) if m else None

def _extract_ix_from_url(u: str) -> Optional[str]:
    try:
        q = parse_qs(urlparse(u).query)
        v = q.get("ix", [None])[0]
        return str(v) if v else None
    except Exception:
        return None

def _parse_day_and_status(li) -> Tuple[Optional[int], Optional[str]]:
    day_div = li.select_one("div.day")
    if not day_div: return None, None
    dday = None
    m = re.search(r"D-?\s*(\d+)", day_div.get_text(" ", strip=True))
    if m: dday = int(m.group(1))
    st = None
    sp = day_div.select_one("span.dday")
    if sp: st = sp.get_text(strip=True)
    else:
        text = day_div.get_text(" ", strip=True)
        if "ë§ˆê°" in text: st = "ë§ˆê°"
    return dday, st

def _parse_item(li) -> Optional[Dict]:
    a = li.select_one("div.tit a")
    if not a: return None
    title = a.get_text(strip=True)
    href  = a.get("href", "")
    url   = urljoin(BASE, href)
    ix    = _extract_ix_from_url(url)

    organizer = li.select_one("div.organ")
    organizer = organizer.get_text(strip=True) if organizer else None

    dday, status = _parse_day_and_status(li)

    views_div = li.select_one("div.read")
    views = _parse_int(views_div.get_text(strip=True)) if views_div else None

    sub = li.select_one("div.sub-tit")
    category = None
    if sub:
        txt = sub.get_text(" ", strip=True)
        category = txt.split(":", 1)[1].strip() if ":" in txt else txt

    return {
        "title": title, "url": url, "ix": ix or url,
        "organizer": organizer, "dday": dday, "status": status,
        "views": views, "category": category,
    }

def scrape_wevity_naming(frm: int, to: int, delay_sec: float) -> List[Dict]:
    out: List[Dict] = []
    for gp in range(frm, to + 1):
        url = f"{BASE}?c=find&s=1&gub=1&cidx=25&gp={gp}"
        html_text = _get_html(url)
        if not html_text:
            time.sleep(delay_sec); continue
        soup = BeautifulSoup(html_text, "html.parser")
        lis = soup.select("div.ms-list ul.list > li")
        items = [li for li in lis if "top" not in (li.get("class") or [])]
        if not items: print(f"[WARN] ëª©ë¡ ë¯¸ê²€ì¶œ. ì„ íƒì í™•ì¸ í•„ìš”: {url}")
        for li in items:
            it = _parse_item(li)
            if it: out.append(it)
        time.sleep(delay_sec)
    return out

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GitHub Contents API
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _api_url() -> str:
    return f"https://api.github.com/repos/{CONFIG['OWNER']}/{CONFIG['REPO']}/contents/{CONFIG['PATH']}"

def _headers() -> Dict:
    return {
        "Authorization": f"Bearer {CONFIG['TOKEN']}",
        "Accept": "application/vnd.github+json",
        "X-GitHub-Api-Version": "2022-11-28",
    }

def _normalize_ix(item: Dict) -> str:
    ix = item.get("ix") or item.get("url")
    if not ix: raise ValueError("ê° í•­ëª©ì—ëŠ” ìµœì†Œ 'ix' ë˜ëŠ” 'url'ì´ í•„ìš”")
    return str(ix).strip()

def _get_current_strict() -> Tuple[str, List[Dict]]:
    params = {"ref": CONFIG["BRANCH"]} if CONFIG.get("BRANCH") else None
    r = requests.get(_api_url(), headers=_headers(), params=params, timeout=20)
    if r.status_code == 404:
        raise FileNotFoundError(
            f"GitHubì— '{CONFIG['PATH']}' íŒŒì¼ì´ ì—†ìŒ (ë¦¬í¬: {CONFIG['OWNER']}/{CONFIG['REPO']}, ë¸Œëœì¹˜: {CONFIG['BRANCH']})"
        )
    r.raise_for_status()
    data = r.json()
    content_b64 = data.get("content", "")
    text = base64.b64decode(content_b64).decode("utf-8") if content_b64 else "[]"
    try: arr = json.loads(text) if text.strip() else []
    except json.JSONDecodeError: arr = []
    return data["sha"], arr

def _put_json(new_list: List[Dict], prev_sha: str, message: str) -> Dict:
    body = {
        "message": message,
        "content": base64.b64encode(
            json.dumps(new_list, ensure_ascii=False, indent=2).encode("utf-8")
        ).decode("utf-8"),
        "sha": prev_sha,
    }
    if CONFIG.get("BRANCH"): body["branch"] = CONFIG["BRANCH"]
    r = requests.put(_api_url(), headers=_headers(), json=body, timeout=25)
    r.raise_for_status()
    return r.json()

def merge_and_detect_new(current: List[Dict], scraped: List[Dict], *, added_at: Optional[str] = None, update_existing: bool = False):
    cur_map = { _normalize_ix(x): x for x in current }
    new_items = []
    for raw in scraped:
        ix = _normalize_ix(raw)
        item = dict(raw); item["ix"] = ix
        if ix not in cur_map:
            if "added_at" not in item and added_at:
                item["added_at"] = added_at
            new_items.append(item)
            cur_map[ix] = item
        elif update_existing:
            preserved_added = cur_map[ix].get("added_at")
            cur_map[ix] = {**cur_map[ix], **item}
            if preserved_added is not None:
                cur_map[ix]["added_at"] = preserved_added

    def sort_key(x):
        s = str(x.get("ix", ""))
        return int(s) if s.isdigit() else -1
    merged = sorted(cur_map.values(), key=sort_key, reverse=True)
    return merged, new_items

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Telegram
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def tg_send_message(token: str, chat_id: str, text: str, *, parse_mode="HTML", disable_preview=False):
    url = f"https://api.telegram.org/bot{token}/sendMessage"
    payload = {
        "chat_id": chat_id,
        "text": text,
        "parse_mode": parse_mode,
        "disable_web_page_preview": disable_preview,  # Falseë©´ ë¯¸ë¦¬ë³´ê¸° ON
    }
    r = requests.post(url, json=payload, timeout=15)
    if r.status_code == 429:
        time.sleep(1.2); r = requests.post(url, json=payload, timeout=15)
    r.raise_for_status()
    return r.json()

def notify_telegram(new_items: List[Dict]):
    if not TG["BOT_TOKEN"] or not TG["CHAT_IDS"]:
        print("[ì•Œë¦¼] TG ì„¤ì • ë¹„ì–´ ìˆìŒ â†’ í…”ë ˆê·¸ë¨ ì „ì†¡ ìƒëµ"); return

    # ì˜¤ë˜ëœ ê²ƒ â†’ ìµœì‹  ìˆœìœ¼ë¡œ ë°œì†¡
    def ix_as_int(x):
        s = str(x.get("ix",""))
        return int(s) if s.isdigit() else 10**12
    ordered = sorted(new_items, key=ix_as_int)

    for it in ordered:
        title = html.escape(it.get("title",""))
        url   = it.get("url","")
        safe_url = html.escape(url, quote=True)

        d     = it.get("dday")
        dtext = f"D-{d}" if isinstance(d, int) else "-"

        organizer = html.escape(it.get("organizer","") or "-")
        status    = html.escape(it.get("status","") or "-")
        views     = it.get("views")
        views_txt = f"{views:,}" if isinstance(views, int) else "-"

        text = (
            f"ğŸ“£ <b>{title}</b>\n\n"
            f"ì£¼ìµœ: {organizer}\n"
            f"ìƒíƒœ: {dtext} â€¢ {status} â€¢ ì¡°íšŒ {views_txt}\n\n"
            f"ğŸ”— <a href=\"{safe_url}\">ê³µê³  ë°”ë¡œê°€ê¸°</a>"
        )
        for cid in TG["CHAT_IDS"]:
            tg_send_message(TG["BOT_TOKEN"], cid, text, disable_preview=False)
            time.sleep(1.05)

def notify_print(new_items: List[Dict]):
    def ix_as_int(x):
        s = str(x.get("ix",""))
        return int(s) if s.isdigit() else 10**12
    ordered = sorted(new_items, key=ix_as_int)
    for it in ordered:
        d = it.get("dday"); dtext = f"D-{d}" if isinstance(d, int) else "-"
        print("\n".join([
            f"[ì‹ ê·œ] {it.get('title','')}",
            f"  â€¢ ì£¼ìµœ: {it.get('organizer','') or '-'}",
            f"  â€¢ ìƒíƒœ: {dtext} â€¢ {it.get('status','') or '-'} â€¢ ì¡°íšŒ { (f'{it.get('views'):,}' if isinstance(it.get('views'),int) else '-') }",
            f"  â€¢ ë§í¬: {it.get('url','')}",
        ])); print("-"*60)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MAIN
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    # ëŒ€ìƒ JSON ë°˜ë“œì‹œ ì¡´ì¬
    try:
        sha, current = _get_current_strict()
    except FileNotFoundError as e:
        print(f"[ì¤‘ë‹¨] ëŒ€ìƒ JSON íŒŒì¼ ì—†ìŒ â†’ {e}")
        print("GitHub UIì—ì„œ ë¨¼ì € data/wevity_naming.json ì— '[]' ì €ì¥"); return
    except requests.HTTPError as e:
        print(f"[ì¤‘ë‹¨] GitHub ìš”ì²­ ì‹¤íŒ¨: {e}"); return

    scraped = scrape_wevity_naming(SCRAPE_PAGE_FROM, SCRAPE_PAGE_TO, SCRAPE_DELAY_SEC)
    if not scraped:
        print("[ì¤‘ë‹¨] í¬ë¡¤ëŸ¬ ê²°ê³¼ ë¹„ì–´ ìˆìŒ"); return

    for it in scraped:
        it["ix"] = it.get("ix") or it.get("url")

    now_kst = datetime.now(timezone.utc).astimezone(timezone(timedelta(hours=9)))
    added_stamp = now_kst.isoformat()

    merged, new_items = merge_and_detect_new(current, scraped, added_at=added_stamp, update_existing=False)

    if new_items:
        notify_print(new_items)
        msg = f"chore: upsert {len(new_items)} new items, total {len(merged)} @ {datetime.now(timezone.utc).isoformat()}"
        try:
            resp = _put_json(merged, _get_current_strict()[0] if False else sha, msg)  # sha ì¬í™œìš©
            print(f"GitHub ì €ì¥ ì™„ë£Œ: {resp.get('content',{}).get('path')} sha={resp.get('content',{}).get('sha')}")
            notify_telegram(new_items)
        except requests.HTTPError as e:
            print(f"[ì¤‘ë‹¨] GitHub ì—…ì„œíŠ¸ ì‹¤íŒ¨: {e}")
    else:
        print("ì‹ ê·œ í•­ëª© ì—†ìŒ (ì»¤ë°‹ ì—†ìŒ)")

if __name__ == "__main__":
    main()
